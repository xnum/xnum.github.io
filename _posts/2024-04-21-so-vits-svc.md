---
layout: post
title: AI Singing Voice Conversion (SVC) 的試玩心得
categories: [閒聊]
description: 純粹記錄過程
---

一時興起想來玩玩看用AI換歌曲的演唱者聲音，想當然不會有現成的，要自己訓練模型才行。

需要準備的煉丹材料大約是總計30分鐘到一小時的歌聲材料，最好乾淨而且覆蓋的音域夠廣．

如果材料裡面有伴奏的話，出來的聲音完全沒法用．如果覆蓋的音域不夠廣，在某些音高模型推理出來的結果就會啞掉．

由於so-vits-svc的中文資源看起來比較多，就先嘗試用這套來做訓練了。

準備資料的過程大約是準備好一些原始音檔，長度可能是5分鐘到10分鐘不等，太長的直播類型影音檔就要先解出音樂檔，切割適當大小後再丟去分離人聲。很長的原始資料會讓分離人聲軟體記憶體耗盡崩潰，一般來說切到10分鐘以內就可以。接著就要請出第一個免費軟體來分離人聲了。

一開始我用的是voice-separate-v0.0.4，但分離出來的效果不是太滿意。看到有人推薦Untimate Vocal Remover 5，發現他的效果不錯，裡面也有數個模型可以選擇。這邊我選的是：

```
Process Method: VR Architecture
Window Size: 320
Aggression setting: 5
VR Model: 5_HP-Karaoke-UVR
```

分離後甚至連其他人聲的和聲都被劃分到伴奏去，分出來的人聲相對正確乾淨許多。

接續就是用slicer切割成比較小的音檔了。如果切割設定不正確，就會看到好幾個數十秒的大檔，要自己慢慢調整到適當的切割參數。我的標準是讓他產生0秒到數十秒的音檔，留下2秒到25秒的音檔。

資料都準備好以後就可以跟著so-vits-svc的教學跑，但是上面有很多選項跟分支路線，我選擇的是：

```
1. 編碼器：用vec768l12
2. 底模：隨便抓的
3. NSF-HIFIGAN：要抓，因為擴散模型很建議練一份
4. F0預測器：用rmvpe
5. 響度崁入：啟用
6. 預處理(preprocess_hubert_f0.py)：使用淺擴散功能
7. 主模型訓練：我訓練到7200步
8. 擴散模型訓練：我訓練到4000步
9. 聚類音色洩漏控制模型：練就對了，完成訓練會自己退出
10. 特徵檢索：同上
11. 針對推理時的擴散模型使用，我覺得k_step在100到300間對於聲音的乾淨程度都不錯，沒有繼續往大的方向試
```

最後推理完成後，出來的聲音檔沒意外的話會聽起來顯得很單薄，這時候就需要用混音軟體稍微再去加強厚度，有點類似卡拉OK的聲音效果。Audacity就能提供一些基本的編輯功能，在聲音效果選擇壓縮器和殘響，套用上去之後可以有點改善，更進階的調整就需要學習混音軟體的使用了，調整完畢之後再把伴奏音軌也加進來，並且適當的調整兩個音軌的音量到比較平衡，一首歌就完成了。

儘管模型訓練出來的擬真程度已經很高，但針對一些比較需要唱腔用腹部發音的歌曲，像是：晴る，就會顯得不自然，尤其是拉長音和換氣的部分會整個消失或破碎。總之推理的結果會缺乏渾厚感或是感情。另外一首歌的強弱大小聲好像也會被犧牲掉，就看要不要再用混音軟體後製處理看看了。反之來說，一些輕快的歌曲，像是：サインはB，基本後製以後就聽不太出差異。

至於訓練的平台，我在runpod上面租了一台雲端機器，使用community cloud，一張RTX3090每小時只要0.26美元，總共花了五個小時做完這些訓練，花費才1美金多。比起用自己的電腦訓練，還要開冷氣降溫，真是實惠多了。但硬碟空間給的有點少，會不小心用滿就是。但他只能預先儲值才能使用，結果預儲值了25美金，裡面還有23美金不能領出來，真是有點浪費錢。